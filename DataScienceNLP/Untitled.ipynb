{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised NLP on /r/datascience Comments\n",
    "\n",
    "This is a notebook for performing unsupervised NLP on the comments from the datascience subreddit \"Weekly Entering & Transitioning Thread\"\n",
    "\n",
    "The most current one being:\n",
    "https://www.reddit.com/r/datascience/comments/m4u0uu/weekly_entering_transitioning_thread_14_mar_2021/\n",
    "\n",
    "This notebook covers all steps of analysis:\n",
    "1. Data Scraping\n",
    "1. Data Cleaning\n",
    "1. Feature Engineering\n",
    "1. Modelling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import praw\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping\n",
    "\n",
    "To collect the comments will make use of the reddit praw API, which allows us to perform simple (but adequate) queries and collect posts and comments in a useful object oriented format!\n",
    "\n",
    "As a note, I store reddit IDs and Sectets in a separate yaml file, you will have to make your own if you plan to copy any code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = open(\"/Users/harry/scrapeReddit/reddit_keys.yml\")\n",
    "parsed_yaml = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "reddit = praw.Reddit(client_id=parsed_yaml[\"client_id\"],                                                                                                                                                                                          \n",
    "                     client_secret=parsed_yaml[\"client_secret\"],                                                                                                                                                                       \n",
    "                     user_agent=parsed_yaml[\"user_agent\"],                                                                                                                                                                                            \n",
    "                     username=parsed_yaml[\"username\"],                                                                                                                                                                                      \n",
    "                     password=parsed_yaml[\"password\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search the datascience subreddit for posts with both \"weekly\" and \"thread\" in the title, and to only keep posts written by the authors \"datascience-bot\" and \"AutoModerator\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOWED_AUTHORS = [\"datascience-bot\", \"AutoModerator\"]\n",
    "all_submissions = []\n",
    "for submission in reddit.subreddit(\"datascience\").search(\"weekly+thread\", limit=1000):\n",
    "    if submission.author not in ALLOWED_AUTHORS:\n",
    "        continue\n",
    "    all_submissions.append(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to parse through all comments in the thread, saving for each comment: the id, url, thread name (a manually parsed string for keeping track), text body, username, comment depth, number of upvotes and number of replies.\n",
    "\n",
    "Number of replies has a little extra work, ensuring to only count replies that aren't from \"datascience-bot\" or \"AutoModerator\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = []\n",
    "\n",
    "for submission in all_submissions:\n",
    "\n",
    "    submission.comments.replace_more(limit=None)\n",
    "\n",
    "    thread_name = submission.title.split(\"|\")[-1]\n",
    "    thread_name = \"\".join(thread_name.split())\n",
    "\n",
    "    for comment in submission.comments.list():\n",
    "        \n",
    "        if comment.author in ALLOWED_AUTHORS:\n",
    "            continue\n",
    "        \n",
    "        comment_dict = {}\n",
    "        comment_dict[\"id\"] = comment.id\n",
    "        comment_dict[\"url\"] = \"reddit.com{}\".format(comment.permalink)\n",
    "        comment_dict[\"thread\"] = thread_name\n",
    "        comment_dict[\"textbody\"] = comment.body\n",
    "        comment_dict[\"username\"] = comment.author\n",
    "        comment_dict[\"depth\"] = comment.depth\n",
    "        comment_dict[\"upvotes\"] = comment.ups\n",
    "\n",
    "        # Only count replies that aren't from the reddit bots\n",
    "        filtered_replies = [r for r in comment.replies if r.author not in ALLOWED_AUTHORS]\n",
    "        comment_dict[\"replies\"] = len(filtered_replies)\n",
    "        \n",
    "        all_comments.append(comment_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to simply parse the list of dicts into a dataframe. Everything looks as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 13630 Raw Comments\n"
     ]
    }
   ],
   "source": [
    "raw_df = pd.DataFrame(all_comments)\n",
    "raw_df = raw_df.set_index('id')\n",
    "\n",
    "print(\"Collected {} Raw Comments\".format(len(raw_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Filtering\n",
    "\n",
    "Now to clean the raw dataframe, removing unuseful or unprofessional comments.\n",
    "1. Remove comments with body \"[Deleted]\"\n",
    "1. Remove comments with negative karma (mostly spam)\n",
    "\n",
    "First a look at how many comments we scraped that were just empty and contain \"[deleted]\", also no meaningful username attatched. We drop these from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>thread</th>\n",
       "      <th>textbody</th>\n",
       "      <th>username</th>\n",
       "      <th>depth</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>replies</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gq86mcu</th>\n",
       "      <td>reddit.com/r/datascience/comments/lzpbaf/weekl...</td>\n",
       "      <td>07Mar2021-14Mar2021</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gq8d9a3</th>\n",
       "      <td>reddit.com/r/datascience/comments/lzpbaf/weekl...</td>\n",
       "      <td>07Mar2021-14Mar2021</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gqdzydq</th>\n",
       "      <td>reddit.com/r/datascience/comments/lzpbaf/weekl...</td>\n",
       "      <td>07Mar2021-14Mar2021</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gokgd4i</th>\n",
       "      <td>reddit.com/r/datascience/comments/lovorx/weekl...</td>\n",
       "      <td>21Feb2021-28Feb2021</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gox8y5a</th>\n",
       "      <td>reddit.com/r/datascience/comments/lovorx/weekl...</td>\n",
       "      <td>21Feb2021-28Feb2021</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       url  \\\n",
       "id                                                           \n",
       "gq86mcu  reddit.com/r/datascience/comments/lzpbaf/weekl...   \n",
       "gq8d9a3  reddit.com/r/datascience/comments/lzpbaf/weekl...   \n",
       "gqdzydq  reddit.com/r/datascience/comments/lzpbaf/weekl...   \n",
       "gokgd4i  reddit.com/r/datascience/comments/lovorx/weekl...   \n",
       "gox8y5a  reddit.com/r/datascience/comments/lovorx/weekl...   \n",
       "\n",
       "                      thread   textbody username  depth  upvotes  replies  \n",
       "id                                                                         \n",
       "gq86mcu  07Mar2021-14Mar2021  [deleted]     None      0        1        1  \n",
       "gq8d9a3  07Mar2021-14Mar2021  [deleted]     None      0        1        1  \n",
       "gqdzydq  07Mar2021-14Mar2021  [deleted]     None      0        1        2  \n",
       "gokgd4i  21Feb2021-28Feb2021  [deleted]     None      0        1        0  \n",
       "gox8y5a  21Feb2021-28Feb2021  [deleted]     None      0        1        1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df[raw_df.textbody.str.contains(\"\\[deleted\\]\")].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Comments Before: 13630\n",
      "Number of Comments After: 12771\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Comments Before: {}\".format(len(raw_df)))\n",
    "raw_df = raw_df[~raw_df.textbody.str.contains(\"\\[deleted\\]\")]\n",
    "print(\"Number of Comments After: {}\".format(len(raw_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we drop any comment with negative karma, this is usually some kind of spam / self promotion. Though this in theory should protect against some unuseful and hostile comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Comments Before: 12771\n",
      "Number of Comments After: 12672\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Comments Before: {}\".format(len(raw_df)))\n",
    "raw_df = raw_df[raw_df[\"upvotes\"] >= 0]\n",
    "print(\"Number of Comments After: {}\".format(len(raw_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this study only comments that have gauged some amount of discussion are kept, and hence filter only those at depth=0 with replies>0. This does remove a large amount of comments, but those remaining should be more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Comments Before: 12672\n",
      "Number of Comments After: 4848\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Comments Before: {}\".format(len(raw_df)))\n",
    "raw_df = raw_df[(raw_df[\"replies\"] >= 0) & (raw_df[\"depth\"] == 0)]\n",
    "print(\"Number of Comments After: {}\".format(len(raw_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is not a requirement, to avoid causing too much interference with the NLP all URLs / hyperlinks are removed from the comment textbody. This regex just matches anything beginning with http or www and replaces it with nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[\"textbody\"] = raw_df[\"textbody\"].str.replace('http\\S+|www.\\S+', '', case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Enginerring\n",
    "\n",
    "The most common methods for encoding a text document for machine learning input are using\n",
    "1. Count Vectorizer [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "1. tf–idf Vectorizer [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "Generally it is preferable to use a tf-idf as this gives a lower weight to words or phrases that will be common to most comments, but give larger weights to what makes the comment unique, and hopefully what the subject is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer\n",
    "\n",
    "The sklearn TfidfVectorizer is fitted on the full set of comments. The removal of english stopwords is used and a vectorizer is constructed for ngrams of size (1, 2 and 3). \n",
    "\n",
    "For visualisation of what these vectorising are doing, will first just show the top 50 features in each vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " == Vectorizer ngram1 Top 50 == \n",
      "['advice', 'analysis', 'analyst', 'analytics', 'background', 'business', 'career', 'company', 'course', 'courses', 'currently', 'data', 'degree', 'doing', 'don', 'ds', 'engineering', 'experience', 'field', 'good', 'help', 'hi', 'job', 'just', 'know', 'learn', 'learning', 'like', 'looking', 'masters', 'need', 'program', 'projects', 'python', 'really', 'school', 'science', 'scientist', 'skills', 'sql', 'statistics', 'thanks', 'think', 'time', 've', 'want', 'work', 'working', 'year', 'years']\n",
      "\n",
      " == Vectorizer ngram1 Top 50 == \n",
      "['best way', 'big data', 'business analytics', 'career data', 'computer science', 'currently working', 'data analysis', 'data analyst', 'data analytics', 'data engineering', 'data science', 'data scientist', 'data scientists', 'deep learning', 'don know', 'don want', 'entry level', 'experience data', 'feel like', 'grad school', 'greatly appreciated', 'hey guys', 'hi guys', 'interested data', 'job data', 'job market', 'learning data', 'learning python', 'level data', 'linear algebra', 'machine learning', 'master degree', 'masters data', 'masters degree', 'ms data', 'online courses', 'python sql', 'science analytics', 'science data', 'science field', 'science job', 'science machine', 'sql python', 'thanks advance', 'time series', 'transition data', 'work data', 'work experience', 'working data', 'years experience']\n",
      "\n",
      " == Vectorizer ngram1 Top 50 == \n",
      "['analytics data science', 'applied data science', 'career data science', 'data analyst data', 'data analyst job', 'data analyst position', 'data analyst role', 'data science analytics', 'data science bootcamp', 'data science career', 'data science course', 'data science currently', 'data science data', 'data science degree', 'data science field', 'data science internship', 'data science job', 'data science jobs', 'data science machine', 'data science masters', 'data science ml', 'data science position', 'data science positions', 'data science program', 'data science related', 'data science role', 'data science ve', 'data scientist position', 'data scientist role', 'degree data science', 'entry level data', 'entry level job', 'experience data science', 'field data science', 'having hard time', 'interested data science', 'job data science', 'learn data science', 'learning data science', 'looking data science', 'machine learning engineer', 'master data science', 'masters data science', 'ms data science', 'msc data science', 'new data science', 'python data science', 'science machine learning', 'statistics data science', 'transition data science']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "N_FEATURES = 50\n",
    "\n",
    "vectorizer_ngram1 = TfidfVectorizer(stop_words='english', max_features = N_FEATURES, ngram_range=(1,1))\n",
    "vectorizer_ngram2 = TfidfVectorizer(stop_words='english', max_features = N_FEATURES, ngram_range=(2,2))\n",
    "vectorizer_ngram3 = TfidfVectorizer(stop_words='english', max_features = N_FEATURES, ngram_range=(3,3))\n",
    "\n",
    "vectorizer_ngram1.fit_transform(raw_df[\"textbody\"])\n",
    "vectorizer_ngram2.fit_transform(raw_df[\"textbody\"])\n",
    "vectorizer_ngram3.fit_transform(raw_df[\"textbody\"])\n",
    "\n",
    "print(\"\\n == Vectorizer ngram1 Top {} == \\n{}\".format(N_FEATURES, vectorizer_ngram1.get_feature_names()))\n",
    "print(\"\\n == Vectorizer ngram1 Top {} == \\n{}\".format(N_FEATURES, vectorizer_ngram2.get_feature_names()))\n",
    "print(\"\\n == Vectorizer ngram1 Top {} == \\n{}\".format(N_FEATURES, vectorizer_ngram3.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see is that ngram=1 and ngram=2 vectorizers do have some noise with a lot of general english language still there. These are expected to recieve low tf-idf weights as they'll likely be common to many comments. They do contain some useful phrases such as individual skillsets like\n",
    "- masters, ml, sql\n",
    "- linear algebra, big data, science degree\n",
    "\n",
    "The ngram=3 vectorizer might be too much noise, but it still appears to pick up some interesting phrases such as\n",
    "- data science bootcamp, machine learning engineer, transition data science\n",
    "\n",
    "\n",
    "To limit the machine learning only to those that will be useful a different number of features were selected for each ngram level. ngram1 = 500, ngram2 = 250, ngram3 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_ngram1 = TfidfVectorizer(stop_words='english', max_features = 500, ngram_range=(1,1))\n",
    "vectorizer_ngram2 = TfidfVectorizer(stop_words='english', max_features = 250, ngram_range=(2,2))\n",
    "vectorizer_ngram3 = TfidfVectorizer(stop_words='english', max_features = 100, ngram_range=(3,3))\n",
    "\n",
    "X = vectorizer_ngram1.fit_transform(raw_df[\"textbody\"])\n",
    "X = vectorizer_ngram2.fit_transform(raw_df[\"textbody\"])\n",
    "X = vectorizer_ngram3.fit_transform(raw_df[\"textbody\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
